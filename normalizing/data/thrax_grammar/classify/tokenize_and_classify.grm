import '../util.grm' as util;
import 'cardinal.grm' as c;
import 'ordinal.grm' as o;
import 'decimal.grm' as dec;
import 'time.grm' as t;
import 'date.grm' as dt;
import 'time_sports.grm' as ts;
import 'sports_results.grm' as sr;
import 'connector.grm' as co;
import 'pron_symbols.grm' as sy;
import 'non_pron_symbols.grm' as ns;
import 'acronyms.grm' as a;
import 'abbreviations.grm' as ab;
import 'nsw.grm' as nsw;
import 'word.grm' as w;
import 'punctuation.grm' as p;

semiotic_class = o.ORDINAL 
					| c.CARDINAL 
					| dec.DECIMAL 
					| t.TIME 
					| dt.DATE 
					| co.CONNECTOR 
					| sy.SYMBOL 
					| ns.NON_PRON_SYMS
					| a.ACRONYM
					| ab.ABBREVIATION
					| nsw.NSW;

# Geyma: sr.SPORTS_RESULTS, ts.TIME_SPORTS

words = w.WORD;

word_token = 
  util.Insert["tokens { "]
  words
  util.Insert[" }"]
;

semclass_token = 
  util.Insert["tokens { "]
  semiotic_class
  util.Insert[" }"]
;

token_plus_punct = 
  ((p.PUNCT util.Insert[" "])*
  word_token
  (util.Insert[" "] p.PUNCT)*) |
  (semclass_token
  (util.Insert[" "] p.PUNCT)*)
;

export TOKENIZE_AND_CLASSIFY = Optimize[token_plus_punct (" " (token_plus_punct|p.PUNCT))*];


